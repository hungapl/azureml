{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning Service example\n",
    "### Deploy and publish scoring pipeline\n",
    "\n",
    "*This notebook shows you how to:*\n",
    "- Build a pipeline using ParallelRunStep or PythonScriptStep\n",
    "- Run pipeline as an experiment\n",
    "- Deploy the pipelie as REST end point and request a run via REST \n",
    "There is a similar [tutorial](https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-pipeline-batch-scoring-classification) in Azure documentation, but this example uses a simpler problem which is faster to run without using tensorflow.  The tutorial also ends after describing how to submit a pipeline run via REST without the instructions for querying run status.  Here, I have find the link to a document (thanks to stackoverflow) and provide the steps on how to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup workspace\n",
    "- Workspace created in Azure Machine Learning service - [howto](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?tabs=python)\n",
    "- Updated the cell below with your workspace details and run the cell to connect to workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "config_file = '_config.json'\n",
    "\n",
    "subscription_id='xxxxx' # Subscription ID of the workspace\n",
    "resource_group='xxxxx' # Resource group of the workspace\n",
    "workspace_name = \"xxxxxx\" # Name of your workspace\n",
    "\n",
    "if os.path.isfile(config_file):\n",
    "    with open(config_file, 'r') as f:\n",
    "        configs = json.load(f)\n",
    "        subscription_id = configs['subscription_id']\n",
    "        resource_group = configs['resource_group']\n",
    "        workspace_name = configs['workspace_name']\n",
    "\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.get(name=workspace_name,\n",
    "               subscription_id=subscription_id,\n",
    "               resource_group=resource_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise\n",
    "- Updated the asset names in the cell below (optional)\n",
    "- Execute to initialise variables, create temp directory for storing files for upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asset names \n",
    "model_dir = 'simple_model'\n",
    "model_name = model_dir\n",
    "model_file = 'model_v{}.json'\n",
    "experiment_name = \"pipeline_simple\"\n",
    "compute_name = \"DS3-cluster\"\n",
    "environment_name = \"test_minimal\"\n",
    "dataset_parent_path = \"data\"\n",
    "dataset_name = experiment_name\n",
    "\n",
    "# Temp local directory for storing created files that will be uploaded to AML for the pipeline\n",
    "local_temp_dir = './_temp'\n",
    "\n",
    "import os \n",
    "local_path_to_model = os.path.join(local_temp_dir, model_dir)\n",
    "if not os.path.isdir(local_path_to_model):\n",
    "    os.makedirs(local_path_to_model)\n",
    "    \n",
    "import azureml.core\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Dataset\n",
    "1. Generate sample input dataset csv file by executing the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./_temp/pipeline_simple_input.csv\n",
    "id,gender,age\n",
    "1,F,30\n",
    "2,F,50\n",
    "3,M,55\n",
    "4,M,23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the Azure ML web interface to register the dataset\n",
    "    - Load your AML workspace in the web browser\n",
    "    - Click on 'Dataset' option\n",
    "    - Use the register action to register this input csv as a dataset in your workspace, use the name 'pipeline_simple' as the dataset name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get reference to input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset \n",
    "input_data = Dataset.get_by_name(ws, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and register model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create model\n",
    "class MyModel:\n",
    "    \n",
    "    def load_model(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            loaded_params = json.load(f)\n",
    "            return MyModel(loaded_params)\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self._params = params\n",
    "    \n",
    "    def predict(self, data):\n",
    "        data['prediction'] = data.age < self._params['age_average']\n",
    "        return data[['id', 'prediction']]\n",
    "\n",
    "    \n",
    "\n",
    "params = {'age_average':40}\n",
    "local_model_path = os.path.join(local_path_to_model, model_file)\n",
    "\n",
    "with open(local_model_path, 'w') as f:\n",
    "    json.dump(params, f)\n",
    "# Test model code\n",
    "mymodel = MyModel.load_model(local_model_path)\n",
    "print(str(mymodel._params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "\n",
    "# checks to see if compute target already exists in workspace, else create it\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=compute_name)\n",
    "except ComputeTargetException as e:\n",
    "    config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_DS3_V2\",\n",
    "                                                   vm_priority=\"lowpriority\", \n",
    "                                                   min_nodes=0, \n",
    "                                                   max_nodes=1)\n",
    "\n",
    "    compute_target = ComputeTarget.create(workspace=ws, name=compute_name, provisioning_configuration=config)\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define environment\n",
    "- Create requirements file with dependencies require for running your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./_temp/requirements.txt\n",
    "pandas\n",
    "numpy\n",
    "azureml-core\n",
    "azureml-dataset-runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "# Pipeline doesn't like using curated environment, create a new one \n",
    "env = Environment.from_pip_requirements(name = environment_name, file_path = os.path.join(local_temp_dir, 'requirements.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build pipeline\n",
    "#### 1. Create scoring file\n",
    "This is a python script for making prediction based on your model.  This [section](https://docs.microsoft.com/en-au/azure/machine-learning/how-to-deploy-existing-model#entry-script-scorepy) in Azure documentation explains how this file works.  Note how this scoring script uses azureml.core modules, hence we need the azureml-core azure-datatime-runtime included as dependencies when defining the environment previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./_temp/scoring.py\n",
    "\n",
    "from azureml.core import Run\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    \n",
    "    class MyModel:\n",
    "\n",
    "        def load_model(file_path):\n",
    "            with open(file_path, 'r') as f:\n",
    "                loaded_params = json.load(f)\n",
    "                return MyModel(loaded_params)\n",
    "\n",
    "        def __init__(self, params):\n",
    "            self._params = params\n",
    "\n",
    "        def predict(self, data):\n",
    "            data['prediction'] = data.age < self._params['age_average']\n",
    "            return data[['id', 'prediction']]\n",
    "        \n",
    "    # Read from parameters from argument, e.g. which model to use\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_name', dest=\"model_name\", required=True)\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    \n",
    "    model_dir = 'simple_model'\n",
    "    model_name = args.model_name\n",
    "    print(str(datetime.now()) + ': init()')\n",
    "\n",
    "    model_path = Model.get_model_path(model_dir) + '/' + model_name + '.json'\n",
    "    model = MyModel.load_model(model_path)\n",
    "    print(str(datetime.now()) + ': Model loaded')\n",
    "\n",
    "\n",
    "def run(data):\n",
    "    print(str(datetime.now()) + ': run()')\n",
    "    output = model.predict(data)\n",
    "    print(str(datetime.now()) + ': run completed()')\n",
    "    return output\n",
    "\n",
    "def local_test():\n",
    "    init()\n",
    "    input_data = pd.read_csv(os.path.join('_temp/pipeline_simple_input.csv'))\n",
    "    print(run(input_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define the pipeline\n",
    "- Here, we are building a pipeline with only one step.  A pipeline can contain multiple steps that perform post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "def_data_store = Datastore.get_default(ws)\n",
    "score_script = os.path.join(local_temp_dir, \"scoring.py\")\n",
    "model_name_param = PipelineParameter(name=\"model_arg\", default_value=model_name)\n",
    "\n",
    "def parallel_run_step():\n",
    "    from azureml.pipeline.steps import ParallelRunConfig\n",
    "    from azureml.pipeline.core import PipelineData\n",
    "\n",
    "    parallel_run_config = ParallelRunConfig(\n",
    "        environment=env,\n",
    "        entry_script=score_script,\n",
    "        source_directory=\".\",\n",
    "        output_action=\"append_row\",\n",
    "        append_row_file_name=\"parallel_run_step.txt\",\n",
    "        compute_target=compute_target,\n",
    "        error_threshold=1,\n",
    "        node_count=1,\n",
    "        process_count_per_node=2\n",
    "    )\n",
    "\n",
    "    from azureml.pipeline.steps import ParallelRunStep\n",
    "    from datetime import datetime\n",
    "\n",
    "    parallel_step_name = \"scoring-\" + datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "    output_dir = PipelineData(name=dataset_name, datastore=def_data_store)\n",
    "    \n",
    "\n",
    "    score_step = ParallelRunStep(\n",
    "        name=parallel_step_name,\n",
    "        inputs=[input_data.as_named_input(\"input\")],\n",
    "        output=output_dir,\n",
    "        arguments=[\"--model_name\", model_name_param],\n",
    "        parallel_run_config=parallel_run_config,\n",
    "        allow_reuse=False\n",
    "    )\n",
    "    return score_step\n",
    "\n",
    "def python_step_step():\n",
    "    return PythonScriptStep(script_name=score_script,\n",
    "                             arguments=[\"--model_name\", model_name_param],\n",
    "                             compute_target=compute_target,\n",
    "                             source_directory=\".\")\n",
    "score_step = python_step_step()\n",
    "pipeline = Pipeline(workspace=ws, steps=[score_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Deploy the pipeline\n",
    "- This will trigger a run (first run usually takes a few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "pipeline_run = Experiment(ws, experiment_name).submit(pipeline)\n",
    "print(pipeline_run)\n",
    "pipeline_run.wait_for_completion(show_output=True)\n",
    "print('Completed in {}'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Deploy as a REST endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(\n",
    "    name=experiment_name + \"_rest\", description=\"scoring\", version=\"1.0\")\n",
    "\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Request to run pipeline via REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_endpoint = published_pipeline.endpoint\n",
    "print(rest_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "interactive_auth = InteractiveLoginAuthentication()\n",
    "auth_header = interactive_auth.get_authentication_header()\n",
    "\n",
    "\n",
    "response = requests.post(rest_endpoint, \n",
    "                         headers=auth_header, \n",
    "                         json={\"ExperimentName\": experiment_name,\n",
    "                               \"ParameterAssignments\": {\"model_name\": model_name}})\n",
    "try:\n",
    "    response.raise_for_status()\n",
    "except Exception:    \n",
    "    raise Exception(\"Received bad response from the endpoint: {}\\n\"\n",
    "                    \"Response Code: {}\\n\"\n",
    "                    \"Headers: {}\\n\"\n",
    "                    \"Content: {}\".format(rest_endpoint, response.status_code, response.headers, response.content))\n",
    "\n",
    "run_id = response.json().get('Id')\n",
    "print('Submitted pipeline run: ', run_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check run status (Reference: [Create, run, and delete Azure ML resources using REST](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-rest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "run_details_url = 'https://{api_server}/history/v1.0/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{workspace_name}/experiments/{experiment_name}/runs/{run_id}/'.format(\n",
    "api_server='westus2.api.azureml.ms',\n",
    "subscription_id=subscription_id,\n",
    "resource_group=resource_group,\n",
    "workspace_name=workspace_name,\n",
    "experiment_name=experiment_name,\n",
    "run_id=run_id\n",
    ")\n",
    "\n",
    "#print(run_details_url)\n",
    "FINISHED_STATUS = ['Completed','Failed','Canceled']\n",
    "\n",
    "finished = False\n",
    "while not finished:\n",
    "    response = requests.get(run_details_url, \n",
    "                             headers=auth_header)\n",
    "    results = json.loads(response.content)\n",
    "    finished = results['status'] in FINISHED_STATUS\n",
    "    print('Run {} completed.'.format(run_id))\n",
    "    print(json.dumps(results, indent=4))\n",
    "    \n",
    "    if not finished:\n",
    "        time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
